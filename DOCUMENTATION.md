# AeroStream - Documentation Technique ComplÃ¨te

## ğŸ“‹ Table des MatiÃ¨res
1. [Vue d'ensemble](#vue-densemble)
2. [Architecture du SystÃ¨me](#architecture-du-systÃ¨me)
3. [Technologies UtilisÃ©es](#technologies-utilisÃ©es)
4. [Concepts ClÃ©s](#concepts-clÃ©s)
5. [Pipeline de DonnÃ©es](#pipeline-de-donnÃ©es)
6. [Composants DÃ©taillÃ©s](#composants-dÃ©taillÃ©s)
7. [Workflow Machine Learning](#workflow-machine-learning)
8. [DÃ©ploiement](#dÃ©ploiement)
9. [Guide d'Installation](#guide-dinstallation)

---

## ğŸ¯ Vue d'ensemble

**AeroStream** est une plateforme d'analyse de sentiments en temps rÃ©el pour les tweets concernant les compagnies aÃ©riennes amÃ©ricaines. Le systÃ¨me collecte, nettoie, analyse et visualise les sentiments des clients en utilisant des techniques avancÃ©es de Machine Learning et NLP.

### Objectifs du Projet
- Analyser les sentiments (positif, neutre, nÃ©gatif) des tweets en temps rÃ©el
- Identifier les principales causes d'insatisfaction client
- Calculer les taux de satisfaction par compagnie aÃ©rienne
- Fournir un tableau de bord interactif pour la visualisation des KPIs
- Automatiser l'ensemble du pipeline via orchestration

### Dataset
- **Source**: Hugging Face - `7Xan7der7/us_airline_sentiment`
- **Volume**: ~14,640 tweets
- **Classes**: Negative, Neutral, Positive (donnÃ©es dÃ©sÃ©quilibrÃ©es)
- **Features**: 13 colonnes incluant texte, compagnie, sentiment, raisons nÃ©gatives

---

## ğŸ—ï¸ Architecture du SystÃ¨me

### Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AEROSTREAM ARCHITECTURE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jupyter    â”‚â”€â”€â”€â”€â”€â–¶â”‚   ChromaDB   â”‚â”€â”€â”€â”€â”€â–¶â”‚    Models    â”‚
â”‚  Notebooks   â”‚      â”‚  (Vector DB) â”‚      â”‚   Training   â”‚
â”‚ (EDA/Prep)   â”‚      â”‚              â”‚      â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                                    â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚         FastAPI Backend             â”‚
                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                      â”‚  â”‚   ML Model (Best Model)     â”‚   â”‚
                      â”‚  â”‚   Sentence Transformers     â”‚   â”‚
                      â”‚  â”‚   Prediction Service        â”‚   â”‚
                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Airflow    â”‚ â”‚PostgreSQLâ”‚  â”‚  Streamlit   â”‚
            â”‚     DAG      â”‚ â”‚ Backend  â”‚  â”‚  Dashboard   â”‚
            â”‚  (ETL Loop)  â”‚ â”‚   DB     â”‚  â”‚   (Viz)      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚             â–²               â–²
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   Micro-batch Pipeline Flow
```

### Flux de DonnÃ©es

1. **Phase d'EntraÃ®nement** (Offline)
   ```
   Hugging Face â†’ EDA â†’ Preprocessing â†’ Embeddings â†’ ChromaDB â†’ Model Training â†’ Best Model
   ```

2. **Phase de Production** (Online - Streaming)
   ```
   Airflow â†’ API (Fake Tweets) â†’ Preprocessing â†’ Prediction â†’ PostgreSQL â†’ Streamlit
   ```

---

## ğŸ’» Technologies UtilisÃ©es

### 1. **Data Science & Machine Learning**

| Technologie | Version | Utilisation |
|------------|---------|-------------|
| **Python** | 3.13+ | Langage principal |
| **Sentence Transformers** | Latest | GÃ©nÃ©ration d'embeddings (paraphrase-multilingual-MiniLM-L12-v2) |
| **Scikit-learn** | Latest | ModÃ¨les ML (Logistic Regression, Random Forest) |
| **XGBoost** | Latest | ModÃ¨le de boosting |
| **PyTorch** | Latest | Backend pour transformers & MLP |
| **Pandas** | Latest | Manipulation de donnÃ©es |
| **NumPy** | Latest | Calculs numÃ©riques |

### 2. **Natural Language Processing**

- **Sentence Transformers**: Embeddings de 384 dimensions
- **Regex (re)**: Nettoyage de texte
- **Emoji**: Conversion Ã©mojis â†’ texte
- **NLTK**: Stopwords et analyse linguistique

### 3. **Bases de DonnÃ©es**

| Database | Type | Usage |
|----------|------|-------|
| **ChromaDB** | Vector Database | Stockage des embeddings (train/test collections) |
| **PostgreSQL** | Relational DB | Stockage des tweets prÃ©dits et mÃ©tadonnÃ©es |

### 4. **Backend & API**

- **FastAPI**: Framework REST API moderne avec support async
- **Uvicorn**: Serveur ASGI haute performance
- **Pydantic**: Validation des schÃ©mas de donnÃ©es
- **Psycopg2**: Driver PostgreSQL

### 5. **Orchestration & Workflow**

- **Apache Airflow**: Orchestration du pipeline ETL
  - LocalExecutor pour exÃ©cution locale
  - DAGs Python pour dÃ©finition du workflow
  - XCom pour communication inter-tÃ¢ches

### 6. **Visualisation**

- **Streamlit**: Dashboard interactif
- **Plotly**: Graphiques interactifs (pie charts, bar charts, time series)
- **SQLAlchemy**: ORM pour requÃªtes SQL

### 7. **Containerisation**

- **Docker**: Containerisation des services
- **Docker Compose**: Orchestration multi-conteneurs
- **Networks**: Isolation rÃ©seau des services

---

## ğŸ“š Concepts ClÃ©s

### 1. **Embeddings SÃ©mantiques**

Les **embeddings** transforment du texte en vecteurs numÃ©riques qui capturent le sens sÃ©mantique.

```python
# ModÃ¨le utilisÃ©: paraphrase-multilingual-MiniLM-L12-v2
# CaractÃ©ristiques:
- Dimensions: 384
- Multilingue (support de 50+ langues)
- OptimisÃ© pour la similaritÃ© sÃ©mantique
- PrÃ©entraÃ®nÃ© sur des paires de paraphrases
```

**Avantages**:
- Capture le contexte et les nuances linguistiques
- Robuste aux variations orthographiques
- GÃ¨re automatiquement la casse (pas besoin de lowercase)

### 2. **Vector Database (ChromaDB)**

ChromaDB est une base de donnÃ©es vectorielle optimisÃ©e pour la recherche de similaritÃ©.

**Collections crÃ©Ã©es**:
- `airline_sentiment_train`: 80% des donnÃ©es (~11,712 documents)
- `airline_sentiment_test`: 20% des donnÃ©es (~2,928 documents)

**MÃ©tadonnÃ©es stockÃ©es**:
```json
{
  "label": "negative",
  "airline": "United"
}
```

### 3. **Class Imbalance Handling**

Le dataset est dÃ©sÃ©quilibrÃ© (majoritÃ© de tweets nÃ©gatifs). Solutions appliquÃ©es:

```python
# Calcul des poids de classe
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y),
    y=y
)

# Application dans les modÃ¨les
- Logistic Regression: class_weight='balanced'
- Random Forest: class_weight='balanced'
- XGBoost: sample_weight=class_weights
- MLP: sample_weight=class_weights
```

### 4. **Micro-batch Processing**

L'architecture utilise un traitement par micro-lots pour simuler le streaming temps rÃ©el.

```python
# Configuration Airflow
batch_size = 20  # 20 tweets par exÃ©cution
schedule_interval = '@hourly'  # ExÃ©cution toutes les heures
```

**Avantages**:
- RÃ©duit la latence
- Optimise l'utilisation des ressources
- Facilite le monitoring et le debugging

**SÃ©lection automatique**: BasÃ©e sur le F1-score (weighted) sur test set

### 6. **ETL Pipeline**

**Extract â†’ Transform â†’ Load**

```python
# Task 1: Extract
fetch_data_from_api() â†’ API call â†’ batch de tweets

# Task 2: Transform  
process_text_data() â†’ Nettoyage + preprocessing

# Task 3: Load
store_in_database() â†’ Bulk insert PostgreSQL
```

---

## ğŸ”„ Pipeline de DonnÃ©es

### Phase 1: PrÃ©paration des DonnÃ©es (Offline)

#### 1.1 EDA (Exploratory Data Analysis)
**Notebook**: `1-EDA.ipynb`

```python
# Analyses effectuÃ©es:
- Distribution des classes (imbalanced)
- Statistiques descriptives (13 colonnes)
- DÃ©tection de doublons (1,041 trouvÃ©s)
- Valeurs manquantes
- Analyse des raisons nÃ©gatives
- Wordclouds par sentiment
```

#### 1.2 Preprocessing
**Notebook**: `2-Preprocessing.ipynb`

**Ã‰tapes de nettoyage**:
```python
def preprocess_text(text):
    # 1. Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # 2. Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    
    # 3. Remove mentions (@username)
    text = re.sub(r'@\w+', '', text)
    
    # 4. Keep hashtag content, remove #
    text = re.sub(r'#(\w+)', r'\1', text)
    
    # 5. Convert emojis to text
    text = emoji.demojize(text)
    text = text.replace(":", " ").replace("_", " ")
    
    # 6. Normalize whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
```

**GÃ©nÃ©ration d'embeddings**:
```python
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
X_embeddings = model.encode(X, show_progress_bar=True, batch_size=32)
# Output: (n_samples, 384) numpy array
```

#### 1.3 ChromaDB Storage
**Notebook**: `3-db-setup.ipynb`

```python
# Train/Test Split
train_idx, test_idx = train_test_split(
    range(len(df)),
    test_size=0.2,
    random_state=42,
    stratify=df['airline_sentiment']
)

# Insertion par batch (5000 documents/batch)
train_collection.add(
    embeddings=batch_embeddings.tolist(),
    documents=batch_df['clean_text'].tolist(),
    metadatas=[...],
    ids=[f"train_{i}" for i in range(...)]
)
```

#### 1.4 Model Training
**Notebook**: `4-Modeling copy.ipynb`

**ModÃ¨les entraÃ®nÃ©s**:

```python
# 1. Logistic Regression
lr_model = LogisticRegression(
    max_iter=1000,
    random_state=42,
    class_weight='balanced',
    C=1.0
)

# 2. Random Forest
rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced',
    max_depth=20,
    n_jobs=-1
)

# 3. XGBoost
xgb_model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    eval_metric='mlogloss',
    n_jobs=-1
)

# 4. MLP
mlp_model = MLPClassifier(
    hidden_layer_sizes=(256, 128, 64),
    activation='relu',
    solver='adam',
    alpha=0.0001,
    batch_size=128,
    learning_rate='adaptive',
    learning_rate_init=0.001,
    max_iter=200,
    random_state=42,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=10
)
```

**Ã‰valuation**:
- Classification reports
- Confusion matrices
- F1-scores (macro, weighted)
- ROC curves & AUC
- Learning curves
- Overfitting analysis (train vs test gap)

### Phase 2: Production Pipeline (Online)

#### 2.1 Airflow DAG
**File**: `airflow/dags/ETL.py`

```python
# DAG Configuration
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=3),
}

# Schedule: Hourly execution
schedule_interval='@hourly'

# Tasks Chain
fetch_data >> process_data >> store_data
```

**Task 1: Fetch Data**
```python
def fetch_data_from_api(**context):
    url = f"{API_BASE_URL}/fake-tweets?batch_size={batch_size}"
    response = requests.get(url, timeout=30)
    tweets = response.json()
    context['task_instance'].xcom_push(key='raw_tweets', value=tweets)
```

**Task 2: Process Data**
```python
def process_text_data(**context):
    tweets = context['task_instance'].xcom_pull(key='raw_tweets', ...)
    processed_data = []
    for tweet in tweets:
        clean_tweet_text = clean_text(tweet.get('text', ''))
        processed_tweet = {
            'airline': tweet.get('airline'),
            'airline_sentiment': tweet.get('airline_sentiment'),
            'negativereason': tweet.get('negativereason'),
            'tweet_created': tweet.get('tweet_created'),
            'text': tweet.get('text', ''),
            'clean_text': clean_tweet_text,
            'processed_at': datetime.now().isoformat()
        }
        processed_data.append(processed_tweet)
    context['task_instance'].xcom_push(key='processed_tweets', ...)
```

**Task 3: Store in Database**
```python
def store_in_database(**context):
    processed_tweets = context['task_instance'].xcom_pull(key='processed_tweets', ...)
    conn = psycopg2.connect(**DB_CONFIG)
    insert_query = """
        INSERT INTO airline_tweets 
        (airline_sentiment, negativereason, airline, text, tweet_created, clean_text)
        VALUES %s
    """
    execute_values(cursor, insert_query, values)
    conn.commit()
```

---

## ğŸ”§ Composants DÃ©taillÃ©s

### 1. Backend FastAPI

**Structure**:
```
backend/
â”œâ”€â”€ main.py                 # API endpoints
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ predict.py         # ML prediction service
â”‚   â””â”€â”€ faker.py           # Fake tweet generator
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ prediction.py      # Pydantic models
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pkl     # Trained model
â”‚   â””â”€â”€ label_encoder.pkl  # Label encoder
â””â”€â”€ database/
    â””â”€â”€ database.py        # DB connection
```

**Endpoints**:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check + DB status |
| `/predict` | POST | Single text prediction |
| `/batch-predict` | POST | Multiple texts prediction |
| `/fake-tweets` | GET | Generate fake tweets |

**Prediction Service**:
```python
class AirlineSentimentService:
    def __init__(self):
        self.model = pickle.load(open(model_path, 'rb'))
        self.label_encoder = pickle.load(open(encoder_path, 'rb'))
        self.transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    def predict(self, text):
        clean_text = self.preprocess_text(text)
        embedding = self.transformer.encode([clean_text])[0].reshape(1, -1)
        prediction = self.model.predict(embedding)[0]
        probabilities = self.model.predict_proba(embedding)[0]
        
        return {
            "predicted_sentiment": sentiment,
            "confidence": float(np.max(probabilities)),
            "probabilities": {...}
        }
```

### 2. PostgreSQL Database

**Schema**:
```sql
CREATE TABLE airline_tweets (
    id SERIAL PRIMARY KEY,
    airline_sentiment VARCHAR(20) NOT NULL,
    negativereason VARCHAR(100),
    airline VARCHAR(50) NOT NULL,
    text TEXT NOT NULL,
    tweet_created TIMESTAMP,
    clean_text TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT check_sentiment CHECK (airline_sentiment IN ('positive', 'negative', 'neutral'))
);

-- Indexes pour performance
CREATE INDEX idx_airline ON airline_tweets(airline);
CREATE INDEX idx_sentiment ON airline_tweets(airline_sentiment);
CREATE INDEX idx_tweet_created ON airline_tweets(tweet_created);
CREATE INDEX idx_created_at ON airline_tweets(created_at);
```

### 3. Streamlit Dashboard

**Structure**:
```
streamlit/
â”œâ”€â”€ app.py              # Main dashboard
â””â”€â”€ sql/
    â””â”€â”€ queries.py      # SQL queries
```

**Pages**:
1. **Tableau de Bord KPI**: KPIs principaux + distributions
2. **Analytics DÃ©taillÃ©es**: Analyses approfondies

**KPIs AffichÃ©s**:
- ğŸ“ Nombre total de tweets
- âœˆï¸ Nombre de compagnies aÃ©riennes
- ğŸ˜ Pourcentage de tweets nÃ©gatifs
- ğŸ“Š Distribution des sentiments (pie chart)
- ğŸ“ˆ Volume par compagnie (bar chart)
- ğŸ˜ Top 10 raisons nÃ©gatives
- ğŸ“… Ã‰volution temporelle (time series)
- ğŸ† Taux de satisfaction par compagnie

**Caching Strategy**:
```python
@st.cache_data(ttl=30)  # Cache for 30 seconds
def fetch_kpi_data():
    # Fetch data from PostgreSQL
    ...

# Manual refresh
if st.button("ğŸ”„ RafraÃ®chir"):
    st.cache_data.clear()
    st.rerun()
```

### 4. Airflow

**Configuration**:
```yaml
# docker-compose.yml
airflow:
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
  depends_on:
    - postgres_airflow
    - backend
```

**DAG Features**:
- Schedule: `@hourly`
- Retry: 2 attempts avec 3 minutes de dÃ©lai
- XCom: Communication entre tÃ¢ches
- Error handling: Try/except avec logging

---

## ğŸ”¬ Workflow Machine Learning

### 1. Feature Engineering

**Text â†’ Embeddings Pipeline**:
```
Raw Text â†’ Preprocessing â†’ Sentence Transformer â†’ 384D Vector
```

**Avantages des embeddings**:
- Capture du contexte sÃ©mantique
- RÃ©duction de dimensionnalitÃ© (vocabulaire â†’ 384 dimensions)
- Transfert learning (modÃ¨le prÃ©-entraÃ®nÃ©)
- Robustesse au bruit

### 2. Model Training

**StratÃ©gie d'entraÃ®nement**:
```python
# 1. Data Split (stratifiÃ©)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Class Balancing
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)

# 3. Training avec cross-validation
for model in [lr, rf, xgb, mlp]:
    model.fit(X_train, y_train, sample_weight=weights)
    
# 4. Evaluation
f1_scores = {model: f1_score(y_test, y_pred, average='weighted') for model in models}

# 5. Selection du meilleur modÃ¨le
best_model = max(models, key=lambda x: f1_scores[x])
```

### 3. Evaluation Metrics

**Metrics utilisÃ©es**:
- **F1-Score (Weighted)**: MÃ©trique principale (gÃ¨re le class imbalance)
- **Accuracy**: PrÃ©cision globale
- **Confusion Matrix**: Distribution des prÃ©dictions
- **ROC-AUC**: Performance par classe
- **Learning Curves**: DÃ©tection d'overfitting

**Overfitting Analysis**:
```python
overfitting_gap = f1_train - f1_test
overfitting_pct = (overfitting_gap / f1_train) * 100
```

### 4. Model Deployment

**Sauvegarde**:
```python
# Best model
with open("../models/best_model.pkl", "wb") as f:
    pickle.dump(best_model, f)

# Label encoder (si nÃ©cessaire)
with open("../models/label_encoder.pkl", "wb") as f:
    pickle.dump(le, f)
```

**Chargement en production**:
```python
# backend/services/predict.py
self.model = pickle.load(open(model_path, 'rb'))
self.label_encoder = pickle.load(open(encoder_path, 'rb'))
self.transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
```

---

## ğŸ³ DÃ©ploiement

### Docker Compose Architecture

**Services**:
```yaml
services:
  postgres_backend:   # Port 5432
  postgres_airflow:   # Port 5434
  backend:            # Port 8000
  airflow:            # Port 8080
  streamlit:          # Port 8501
```

**Network**:
```yaml
networks:
  aerostream:
    driver: bridge
```

**Volumes**:
```yaml
volumes:
  postgres_backend_data:  # Persistence PostgreSQL backend
  postgres_airflow_data:  # Persistence PostgreSQL Airflow
  airflow-logs:           # Logs Airflow
```

### Service Configuration

#### Backend (FastAPI)
```dockerfile
FROM python:3.13-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

#### Airflow
```dockerfile
FROM apache/airflow:2.7.0-python3.11
USER root
# Install dependencies
USER airflow
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
```

#### Streamlit
```dockerfile
FROM python:3.13-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

### Health Checks

```yaml
postgres_backend:
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U ali -d backend_db"]
    interval: 10s
    timeout: 5s
    retries: 5

postgres_airflow:
  healthcheck:
    test: ["CMD", "pg_isready", "-U", "airflow"]
    interval: 5s
    retries: 5
```

---

## ğŸ“¦ Guide d'Installation

### PrÃ©requis
- Docker Desktop
- Docker Compose
- 8GB RAM minimum
- 10GB espace disque

### Installation

```bash
# 1. Clone le repository
git clone <repo_url>
cd AeroStream

# 2. CrÃ©er les rÃ©pertoires nÃ©cessaires
mkdir -p data/raw data/processed data/embedding data/metadata
mkdir -p models chromadb

# 3. Lancer les services
docker-compose up --build

# 4. VÃ©rifier les services
# - Airflow: http://localhost:8080 (admin/admin)
# - FastAPI: http://localhost:8000/docs
# - Streamlit: http://localhost:8501
```

### PrÃ©paration des DonnÃ©es (PremiÃ¨re fois)

```bash
# 1. ExÃ©cuter les notebooks dans l'ordre:
1-EDA.ipynb           # Analyse exploratoire
2-Preprocessing.ipynb # Nettoyage + embeddings
3-db-setup.ipynb      # ChromaDB storage
4-Modeling.ipynb      # Training + evaluation

# 2. VÃ©rifier que les fichiers sont crÃ©Ã©s:
- data/processed/data.csv
- data/embedding/embeddings.npy
- data/metadata/metadata.csv
- models/best_model.pkl
- chromadb/ (collections)

# 3. Initialiser la base PostgreSQL
# â†’ Automatique au dÃ©marrage via init.sql

# 4. Activer le DAG Airflow
# â†’ Aller sur http://localhost:8080
# â†’ Activer "airline_sentiment_etl_pipeline"
```

### Configuration

**Variables d'environnement** (`.env`):
```env
# PostgreSQL Backend
POSTGRES_USER=ali
POSTGRES_PASSWORD=root
POSTGRES_DB=backend_db

# Airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__WEBSERVER__SECRET_KEY=aliali

# API
API_PORT=8000
BACKEND_URL=http://backend:8000
```

---

## ğŸ“Š Monitoring & ObservabilitÃ©

### Logs

**Airflow Logs**:
```bash
docker exec -it airflow bash
tail -f /opt/airflow/logs/dag_id=airline_sentiment_etl_pipeline/...
```

**Backend Logs**:
```bash
docker logs -f fastapi_backend
```

**Streamlit Logs**:
```bash
docker logs -f streamlit
```

### MÃ©triques ClÃ©s

**Pipeline Health**:
- Taux de succÃ¨s des DAG runs
- Temps d'exÃ©cution moyen par task
- Nombre de tweets traitÃ©s/heure

**Model Performance**:
- Distribution des prÃ©dictions
- Confidence scores moyens
- Latence de prÃ©diction

**Database**:
- Nombre total de tweets stockÃ©s
- Taux de croissance
- RequÃªtes lentes

---

## ğŸ”’ SÃ©curitÃ© & Best Practices

### SÃ©curitÃ©

1. **Credentials**: Utiliser des secrets managers (pas de hardcode)
2. **Network Isolation**: Services isolÃ©s sur rÃ©seau Docker privÃ©
3. **CORS**: ConfigurÃ© sur FastAPI (Ã  restreindre en production)
4. **SQL Injection**: Utilisation de parameterized queries
5. **Health Checks**: Surveillance continue des services

### Best Practices

1. **Code Quality**:
   - Type hints en Python
   - Docstrings pour fonctions
   - Error handling systÃ©matique

2. **Data Validation**:
   - Pydantic schemas pour API
   - Constraints SQL sur PostgreSQL
   - Validation des embeddings

3. **Performance**:
   - Batch processing (micro-batches)
   - Caching Streamlit (TTL 30s)
   - Indexes PostgreSQL
   - Connection pooling SQLAlchemy

4. **ScalabilitÃ©**:
   - Architecture microservices
   - Stateless API (FastAPI)
   - Horizontal scaling possible

---

## ğŸš€ AmÃ©liorations Futures

### Court Terme
- [ ] Ajouter auto-refresh automatique Streamlit (sans bouton)
- [ ] Changer schedule Airflow Ã  `*/1 * * * *` (chaque minute)
- [ ] Ajouter monitoring Prometheus/Grafana
- [ ] Tests unitaires et d'intÃ©gration

### Moyen Terme
- [ ] DÃ©ploiement cloud (AWS/GCP/Azure)
- [ ] CI/CD avec GitHub Actions
- [ ] MLflow pour tracking des expÃ©riences
- [ ] A/B testing de modÃ¨les
- [ ] Real-time streaming avec Kafka

### Long Terme
- [ ] Fine-tuning du modÃ¨le transformer
- [ ] Multi-label classification (Ã©motions)
- [ ] DÃ©tection d'anomalies
- [ ] SystÃ¨me de recommandations

---

## ğŸ“– RÃ©fÃ©rences

### Documentation Technique
- [Sentence Transformers](https://www.sbert.net/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Streamlit Documentation](https://docs.streamlit.io/)

### Papers & Resources
- BERT: Pre-training of Deep Bidirectional Transformers
- Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
- XGBoost: A Scalable Tree Boosting System

---