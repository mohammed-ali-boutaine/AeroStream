{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a464e64",
   "metadata": {},
   "source": [
    "Entraînement des modèles : Récupération des embeddings pour entraîner les modèles de classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab224b",
   "metadata": {},
   "source": [
    "Évaluation et sauvegarde du modèle : Évaluation des performances des modèles et conservation du meilleur modèle pour la prédiction future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ffae7",
   "metadata": {},
   "source": [
    "5. Choose and Train Model\n",
    "Start simple, then advance:\n",
    "\n",
    "Baseline: Logistic Regression or Naive Bayes with TF-IDF\n",
    "Mid-level: LSTM or CNN neural networks\n",
    "Advanced: Fine-tune pre-trained models like BERT, DistilBERT, or RoBERTa\n",
    "\n",
    "6. Evaluate Performance\n",
    "Track these metrics:\n",
    "\n",
    "Accuracy\n",
    "Precision, Recall, F1-score per class\n",
    "Confusion matrix to see misclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fdd4a",
   "metadata": {},
   "source": [
    "7. Iterate and Improve\n",
    "\n",
    "Add more training data for weak categories\n",
    "Try data augmentation (paraphrasing)\n",
    "Adjust model hyperparameters\n",
    "Handle class imbalance with weighted loss or oversampling\n",
    "\n",
    "8. Deploy\n",
    "\n",
    "Save your trained model\n",
    "Create an API or interface to classify new tweets\n",
    "Monitor performance on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30393efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_embeddings\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_embeddings, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3218c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# === MODEL 1: Logistic Regression ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1: Logistic Regression\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(f\"\\nWeighted F1-Score: {f1_score(y_test, y_pred_lr, average='weighted'):.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['negative', 'neutral', 'positive'],\n",
    "            yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.title('Logistic Regression - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === MODEL 2: Random Forest ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: Random Forest\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(f\"\\nWeighted F1-Score: {f1_score(y_test, y_pred_rf, average='weighted'):.4f}\")\n",
    "\n",
    "# === MODEL 3: SVM (Optional - slower) ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 3: SVM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=42, class_weight='balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(f\"\\nWeighted F1-Score: {f1_score(y_test, y_pred_svm, average='weighted'):.4f}\")\n",
    "\n",
    "# === COMPARE MODELS ===\n",
    "models = {\n",
    "    'Logistic Regression': f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'Random Forest': f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "    'SVM': f1_score(y_test, y_pred_svm, average='weighted')\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON (Weighted F1-Score)\")\n",
    "print(\"=\"*60)\n",
    "for model_name, score in sorted(models.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model_name}: {score:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "best_model_name = max(models, key=models.get)\n",
    "best_model = {'Logistic Regression': lr_model, 'Random Forest': rf_model, 'SVM': svm_model}[best_model_name]\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "with open(\"../models/best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"\\n✅ Best model ({best_model_name}) saved to ../models/best_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
